{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6112cdd9-82ab-4139-9347-31c05a42fb0b",
   "metadata": {},
   "source": [
    "# How to use NeuralDE's DiffusionEnhancer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a000e-59b3-46d7-b6d7-e07c29b88a25",
   "metadata": {},
   "source": [
    "## Load pretrained models and attacked images\n",
    "\n",
    "First we load a diffusion probabilistic model (2) pretrained in the Imagenet dataset. Due to its size, we use Hugging Face's accelerator library to easily manage the use of the available GPUs. To run this notebook, we recommend a 32 GB GPU with cuda toolkit 11 or higher installed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c16f7c-4519-4042-97cc-f6db0fc3b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import time\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "#import numpy as np\n",
    "#np.random.seed(42)\n",
    "#import torch\n",
    "#torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b528ae3c-56b2-4058-b458-6e60e46a8bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-06 17:34:50.742958: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-06 17:34:50.758433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741282490.776288    1657 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741282490.781648    1657 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-06 17:34:50.799431: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/jovyan/Maturation/env-testneural_github312/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[03-06 17:34:53] {/home/jovyan/Maturation/env-testneural_github312/lib/python3.12/site-packages/neural_de/utils/_twe_logger.py:123} INFO - Logger: name: neural_de_logger, handlers: [<StreamHandler stdout (DEBUG)>]\n",
      "[03-06 17:34:57] {/home/jovyan/Maturation/env-testneural_github312/lib/python3.12/site-packages/neural_de/utils/_minio.py:67} INFO - Model already available locally, skipping download\n",
      "[03-06 17:34:57] {/home/jovyan/Maturation/env-testneural_github312/lib/python3.12/site-packages/neural_de/transformations/_diffusion/_rev_guided_diffusion.py:31} INFO - Building DiffPure model\n",
      "[03-06 17:34:57] {/home/jovyan/Maturation/env-testneural_github312/lib/python3.12/site-packages/neural_de/transformations/_diffusion/_rev_guided_diffusion.py:32} DEBUG - Model Diffpure loaded with config : DiffPureConfig(weights_path=PosixPath('/home/jovyan/.neuralde/diffpure/256x256_diffusion_uncond.pt'), img_shape=(3, 256, 256), attention_resolutions=[32, 16, 8], num_classes=None, dims=2, learn_sigma=True, num_channels=256, num_head_channels=64, num_res_blocks=2, resblock_updown=True, use_fp16=True, use_scale_shift_norm=True, num_heads=4, num_heads_upsample=-1, channel_mult=None, dropout=0.0, use_new_attention_order=False, t=15, t_delta=15, use_bm=False, use_checkpoint=False, conv_resample=True, sample_step=1, rand_t=False)\n",
      "[03-06 17:35:01] {/home/jovyan/Maturation/env-testneural_github312/lib/python3.12/site-packages/neural_de/transformations/_diffusion/_rev_guided_diffusion.py:50} INFO - Loading DiffPure weights to device : cuda\n"
     ]
    }
   ],
   "source": [
    "from neural_de.transformations import DiffusionEnhancer\n",
    "from neural_de.transformations._diffusion._diffpure_config import DiffPureConfig\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "#from accelerate import Accelerator\n",
    "\n",
    "#accelerator = Accelerator()\n",
    "#device = accelerator.device\n",
    "#device = \"cpu\"\n",
    "# Load ADVpurifier\n",
    "config= DiffPureConfig()\n",
    "config.t = 15\n",
    "device = \"cuda\"\n",
    "purifier = DiffusionEnhancer(device=device, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34afc5-5ca0-4807-801b-5252183f0614",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Now we load a pre-trained model of the welding classifier that outputs class 0 for normal and 1 to non compliante weldings respectively. Previously, we have selected randomly a group of 128 images from the test set, and attacked them using the standard version of the autoattack library (3). For convenience the images, and their attacked versions are stored in Pytorch tensors.\n",
    "\n",
    "As will be noted, the accuracy of the classifier drops from 95.31% in the original images to 0% after the adversarial attack. This evidences the success of the method to trick the classifier making him predict defectuous weldings as conformal, and the opposite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d2ff9c-c94f-4c41-b8f6-6c335bb0d2c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../neuralde_renault/pretrained_classifier/resnet_renault_epoch_4.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the classifier pretrained weights\u001b[39;00m\n\u001b[32m      5\u001b[39m weights_dir = \u001b[33m'\u001b[39m\u001b[33m../neuralde_renault/pretrained_classifier/resnet_renault_epoch_4.pth\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m classifier = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.to(\"cpu\")\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Load the batch of 128 images and its attacked versions\u001b[39;00m\n\u001b[32m      9\u001b[39m X_attacked, Y_attacked = torch.load(\u001b[33m'\u001b[39m\u001b[33m../neuralde_renault/data/attacked_X.pth\u001b[39m\u001b[33m'\u001b[39m).to(device), torch.load(\u001b[33m'\u001b[39m\u001b[33m../neuralde_renault/data/attacked_Y.pth\u001b[39m\u001b[33m'\u001b[39m).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Maturation/env-testneural_github312/lib/python3.12/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1422\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1430\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Maturation/env-testneural_github312/lib/python3.12/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    749\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    750\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Maturation/env-testneural_github312/lib/python3.12/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../neuralde_renault/pretrained_classifier/resnet_renault_epoch_4.pth'"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model\n",
    "import torch\n",
    "\n",
    "# Load the classifier pretrained weights\n",
    "weights_dir = '../neuralde_renault/pretrained_classifier/resnet_renault_epoch_4.pth'\n",
    "classifier = torch.load(weights_dir,  map_location=torch.device(device))#.to(\"cpu\")\n",
    "\n",
    "# Load the batch of 128 images and its attacked versions\n",
    "X_attacked, Y_attacked = torch.load('../neuralde_renault/data/attacked_X.pth').to(device), torch.load('../neuralde_renault/data/attacked_Y.pth').to(device)\n",
    "X, Y = torch.load('../neuralde_renault/data/X.pth').to(device), torch.load('../neuralde_renault/data/Y.pth').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ff3bb-9f1c-4c9a-a7b4-934a766cdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoattack import AutoAttack\n",
    "\n",
    "# Create instance of standard attacks\n",
    "attack = AutoAttack(classifier.eval(), device=device)\n",
    "\n",
    "# Caculate the accuracy of the model in the clean and attacked images\n",
    "original_acc = attack.clean_accuracy(X, Y)\n",
    "attacked_acc = attack.clean_accuracy(X_attacked, Y_attacked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97db72-d0a2-4134-9610-7738c9c39882",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Now let's visualize an example of the original and attacked images. As can be seen in the following plot, the adversarial attack is imperceptible to the human eye. Nevertheless, it tricks the classifier reducing its accuracy to 0%. The accuracy shown has been calculated using the whole batch of 128 test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1db6a225-af98-48e3-a33e-aec6ce5b9054",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_acc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m fig, axs = plt.subplots(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m5\u001b[39m))  \u001b[38;5;66;03m# Adjust figsize as needed\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m titles = [\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33moriginal acc. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43moriginal_acc\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mattacked acc. \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattacked_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, img \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m([X[\u001b[32m0\u001b[39m].permute(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m0\u001b[39m).detach().cpu(),\n\u001b[32m      7\u001b[39m                          X_attacked[\u001b[32m0\u001b[39m].permute(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m,\u001b[32m0\u001b[39m).detach().cpu()]):\n\u001b[32m      9\u001b[39m     axs[i].imshow(img)\n",
      "\u001b[31mNameError\u001b[39m: name 'original_acc' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))  # Adjust figsize as needed\n",
    "titles = [f'original acc. {original_acc}%', f'attacked acc. {attacked_acc}%']\n",
    "\n",
    "for i, img in enumerate([X[0].permute(1,2,0).detach().cpu(),\n",
    "                         X_attacked[0].permute(1,2,0).detach().cpu()]):\n",
    "    \n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off')  # Turn off axis\n",
    "    axs[i].set_title(titles[i])  # Set title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680fb91-23b6-4b73-ad7d-5a9ec8187ac2",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Now let's apply the diffpure technique to the attacked images. If the purification is sucessfull, the accuracy of the classifier should sharply increase. This means that the adversarial noise was not only removed, but the image preseves the right semantic to be recognizable by the classifier.\n",
    "\n",
    "After using the diffpure architecture the classifier accuracy is increased from 0% to 77.34%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f02d3c-7d64-485d-b808-aabb4ec362ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dataloader from the attacked images\n",
    "attacked_data = TensorDataset(X_attacked, Y_attacked)\n",
    "attacked_loader = DataLoader(attacked_data, batch_size=1)  # set the batch size according to your hardware capacities\n",
    "\n",
    "# Wrap the models and dataloaders using the accelerator class so it takes care of the GPU managment\n",
    "#purifier, attacked_loader, classifier = accelerator.prepare(purifier, attacked_loader, classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a36a0-6770-41bc-bb8e-c2a8f92e10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purify the images iterating on the batch loader\n",
    "purified_images = []\n",
    "i = 0\n",
    "for batch in tqdm(attacked_loader):\n",
    "    if i< 1:\n",
    "        inputs, targets = batch\n",
    "        outputs = purifier.transform(inputs)\n",
    "        purified_images.append(outputs)\n",
    "        i += 1\n",
    "# Stack the purified images in a single object.\n",
    "purified_images = torch.vstack(purified_images)\n",
    "\n",
    "# Calculate the acurracy of the purified images\n",
    "purified_acc = attack.clean_accuracy(purified_images, Y)*100\n",
    "print(f'Accuracy after purification {purified_acc}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36935d-412d-41ff-8f6a-a6f5192576de",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Upon visual inspection, it is observed that the purified image appears less sharp than the original. This is potentially due to the purification process introducing noisy artifacts altering the real texture of the welding and the material. However, despite these details, the overall semantic integrity of the image, including the size of the weld and its contact points, is preserved. The accuracies reported are calculated on the whole 128 test image batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af851a-8f03-4344-8015-b20b944bfd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "titles = [f'original. Acc {original_acc}%', f'purified. Acc {purified_acc}%', f'attacked. Acc {attacked_acc}%']\n",
    "\n",
    "for i, img in enumerate([X[0].permute(1,2,0).detach().cpu(),\n",
    "                         purified_images[0].permute(1,2,0).detach().cpu(),\n",
    "                         X_attacked[0].permute(1,2,0).detach().cpu()]):\n",
    "    \n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off') \n",
    "    axs[i].set_title(titles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e4518-cf20-4329-8ed3-2997ccb5774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's predictions on the original data\n",
    "original_outputs = classifier(X)\n",
    "original_outputs.argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a584f0d-f415-4c3f-8286-5e8e75d90609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's predictions on the attacked data\n",
    "attacked_outputs = classifier(X_attacked)\n",
    "attacked_outputs.argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1642c-b9df-494d-ad07-895f67f39f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y true labels\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b0934-46ad-4089-a4c9-5fa5f3dd8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's predictions on the purified data\n",
    "classifier = classifier.eval()\n",
    "purified_outputs = classifier(purified_images)\n",
    "purified_outputs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1b8ee-b340-42ec-ae99-714a1ba73d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "end=time.time()\n",
    "print(\"temps final : \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792887b-9b3e-46ac-9535-8b2cda9b10a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-neural_de-etf",
   "language": "python",
   "name": "env-neural_de-etf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
