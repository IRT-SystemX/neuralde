{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6112cdd9-82ab-4139-9347-31c05a42fb0b",
   "metadata": {},
   "source": [
    "# How to use NeuralDE's DiffusionEnhancer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a000e-59b3-46d7-b6d7-e07c29b88a25",
   "metadata": {},
   "source": [
    "## Load pretrained models and attacked images\n",
    "\n",
    "First we load a diffusion probabilistic model (2) pretrained in the Imagenet dataset. Due to its size, we use Hugging Face's accelerator library to easily manage the use of the available GPUs. To run this notebook, we recommend a 32 GB GPU with cuda toolkit 11 or higher installed. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72c16f7c-4519-4042-97cc-f6db0fc3b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import time\n",
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "#import numpy as np\n",
    "#np.random.seed(42)\n",
    "#import torch\n",
    "#torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b528ae3c-56b2-4058-b458-6e60e46a8bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-28 10:28:09.199224: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-28 10:28:09.355465: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-28 10:28:09.389344: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-28 10:28:09.950493: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jovyan/Maturation/neural_DE/env-neural_de/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-02-28 10:28:09.950561: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jovyan/Maturation/neural_DE/env-neural_de/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-02-28 10:28:09.950569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "2025-02-28 10:28:10.781010: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jovyan/Maturation/neural_DE/env-neural_de/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2025-02-28 10:28:10.781062: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "/home/jovyan/Maturation/neural_DE/env-neural_de/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[02-28 10:28:11] {/home/jovyan/Maturation/neural_DE/neuralde/examples/../neural_de/utils/_twe_logger.py:123} INFO - Logger: name: neural_de_logger, handlers: [<StreamHandler stdout (DEBUG)>]\n",
      "[02-28 10:28:15] {/home/jovyan/Maturation/neural_DE/neuralde/examples/../neural_de/utils/_minio.py:67} INFO - Model already available locally, skipping download\n",
      "[02-28 10:28:15] {/home/jovyan/Maturation/neural_DE/neuralde/examples/../neural_de/transformations/_diffusion/_rev_guided_diffusion.py:31} INFO - Building DiffPure model\n",
      "[02-28 10:28:15] {/home/jovyan/Maturation/neural_DE/neuralde/examples/../neural_de/transformations/_diffusion/_rev_guided_diffusion.py:32} DEBUG - Model Diffpure loaded with config : DiffPureConfig(weights_path=PosixPath('/home/jovyan/.neuralde/diffpure/256x256_diffusion_uncond.pt'), img_shape=(3, 256, 256), attention_resolutions=[32, 16, 8], num_classes=None, dims=2, learn_sigma=True, num_channels=256, num_head_channels=64, num_res_blocks=2, resblock_updown=True, use_fp16=True, use_scale_shift_norm=True, num_heads=4, num_heads_upsample=-1, channel_mult=None, dropout=0.0, use_new_attention_order=False, t=15, t_delta=15, use_bm=False, use_checkpoint=False, conv_resample=True, sample_step=1, rand_t=False)\n",
      "[02-28 10:28:18] {/home/jovyan/Maturation/neural_DE/neuralde/examples/../neural_de/transformations/_diffusion/_rev_guided_diffusion.py:50} INFO - Loading DiffPure weights to device : cpu\n"
     ]
    }
   ],
   "source": [
    "from neural_de.transformations import DiffusionEnhancer\n",
    "from neural_de.transformations._diffusion._diffpure_config import DiffPureConfig\n",
    "\n",
    "\n",
    "start=time.time()\n",
    "#from accelerate import Accelerator\n",
    "\n",
    "#accelerator = Accelerator()\n",
    "#device = accelerator.device\n",
    "#device = \"cpu\"\n",
    "# Load ADVpurifier\n",
    "config= DiffPureConfig()\n",
    "config.t = 15\n",
    "device = \"cpu\"\n",
    "purifier = DiffusionEnhancer(device=device, config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a34afc5-5ca0-4807-801b-5252183f0614",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Now we load a pre-trained model of the welding classifier that outputs class 0 for normal and 1 to non compliante weldings respectively. Previously, we have selected randomly a group of 128 images from the test set, and attacked them using the standard version of the autoattack library (3). For convenience the images, and their attacked versions are stored in Pytorch tensors.\n",
    "\n",
    "As will be noted, the accuracy of the classifier drops from 95.31% in the original images to 0% after the adversarial attack. This evidences the success of the method to trick the classifier making him predict defectuous weldings as conformal, and the opposite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d2ff9c-c94f-4c41-b8f6-6c335bb0d2c6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../neuralde_renault/pretrained_classifier/resnet_renault_epoch_4.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the classifier pretrained weights\u001b[39;00m\n\u001b[1;32m      5\u001b[0m weights_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../neuralde_renault/pretrained_classifier/resnet_renault_epoch_4.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.to(\"cpu\")\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Load the batch of 128 images and its attacked versions\u001b[39;00m\n\u001b[1;32m      9\u001b[0m X_attacked, Y_attacked \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../neuralde_renault/data/attacked_X.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../neuralde_renault/data/attacked_Y.pth\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Maturation/neural_DE/env-neural_de/lib/python3.10/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/Maturation/neural_DE/env-neural_de/lib/python3.10/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/Maturation/neural_DE/env-neural_de/lib/python3.10/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../neuralde_renault/pretrained_classifier/resnet_renault_epoch_4.pth'"
     ]
    }
   ],
   "source": [
    "# Load a pre-trained model\n",
    "import torch\n",
    "\n",
    "# Load the classifier pretrained weights\n",
    "weights_dir = '../neuralde_renault/pretrained_classifier/resnet_renault_epoch_4.pth'\n",
    "classifier = torch.load(weights_dir,  map_location=torch.device(device))#.to(\"cpu\")\n",
    "\n",
    "# Load the batch of 128 images and its attacked versions\n",
    "X_attacked, Y_attacked = torch.load('../neuralde_renault/data/attacked_X.pth').to(device), torch.load('../neuralde_renault/data/attacked_Y.pth').to(device)\n",
    "X, Y = torch.load('../neuralde_renault/data/X.pth').to(device), torch.load('../neuralde_renault/data/Y.pth').to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016ff3bb-9f1c-4c9a-a7b4-934a766cdee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoattack import AutoAttack\n",
    "\n",
    "# Create instance of standard attacks\n",
    "attack = AutoAttack(classifier.eval(), device=device)\n",
    "\n",
    "# Caculate the accuracy of the model in the clean and attacked images\n",
    "original_acc = attack.clean_accuracy(X, Y)\n",
    "attacked_acc = attack.clean_accuracy(X_attacked, Y_attacked)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f97db72-d0a2-4134-9610-7738c9c39882",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Now let's visualize an example of the original and attacked images. As can be seen in the following plot, the adversarial attack is imperceptible to the human eye. Nevertheless, it tricks the classifier reducing its accuracy to 0%. The accuracy shown has been calculated using the whole batch of 128 test images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db6a225-af98-48e3-a33e-aec6ce5b9054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))  # Adjust figsize as needed\n",
    "titles = [f'original acc. {original_acc}%', f'attacked acc. {attacked_acc}%']\n",
    "\n",
    "for i, img in enumerate([X[0].permute(1,2,0).detach().cpu(),\n",
    "                         X_attacked[0].permute(1,2,0).detach().cpu()]):\n",
    "    \n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off')  # Turn off axis\n",
    "    axs[i].set_title(titles[i])  # Set title for each subplot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680fb91-23b6-4b73-ad7d-5a9ec8187ac2",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Now let's apply the diffpure technique to the attacked images. If the purification is sucessfull, the accuracy of the classifier should sharply increase. This means that the adversarial noise was not only removed, but the image preseves the right semantic to be recognizable by the classifier.\n",
    "\n",
    "After using the diffpure architecture the classifier accuracy is increased from 0% to 77.34%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f02d3c-7d64-485d-b808-aabb4ec362ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create a dataloader from the attacked images\n",
    "attacked_data = TensorDataset(X_attacked, Y_attacked)\n",
    "attacked_loader = DataLoader(attacked_data, batch_size=1)  # set the batch size according to your hardware capacities\n",
    "\n",
    "# Wrap the models and dataloaders using the accelerator class so it takes care of the GPU managment\n",
    "#purifier, attacked_loader, classifier = accelerator.prepare(purifier, attacked_loader, classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6a36a0-6770-41bc-bb8e-c2a8f92e10b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Purify the images iterating on the batch loader\n",
    "purified_images = []\n",
    "i = 0\n",
    "for batch in tqdm(attacked_loader):\n",
    "    if i< 1:\n",
    "        inputs, targets = batch\n",
    "        outputs = purifier.transform(inputs)\n",
    "        purified_images.append(outputs)\n",
    "        i += 1\n",
    "# Stack the purified images in a single object.\n",
    "purified_images = torch.vstack(purified_images)\n",
    "\n",
    "# Calculate the acurracy of the purified images\n",
    "purified_acc = attack.clean_accuracy(purified_images, Y)*100\n",
    "print(f'Accuracy after purification {purified_acc}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c36935d-412d-41ff-8f6a-a6f5192576de",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "Upon visual inspection, it is observed that the purified image appears less sharp than the original. This is potentially due to the purification process introducing noisy artifacts altering the real texture of the welding and the material. However, despite these details, the overall semantic integrity of the image, including the size of the weld and its contact points, is preserved. The accuracies reported are calculated on the whole 128 test image batch.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5af851a-8f03-4344-8015-b20b944bfd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "titles = [f'original. Acc {original_acc}%', f'purified. Acc {purified_acc}%', f'attacked. Acc {attacked_acc}%']\n",
    "\n",
    "for i, img in enumerate([X[0].permute(1,2,0).detach().cpu(),\n",
    "                         purified_images[0].permute(1,2,0).detach().cpu(),\n",
    "                         X_attacked[0].permute(1,2,0).detach().cpu()]):\n",
    "    \n",
    "    axs[i].imshow(img)\n",
    "    axs[i].axis('off') \n",
    "    axs[i].set_title(titles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874e4518-cf20-4329-8ed3-2997ccb5774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's predictions on the original data\n",
    "original_outputs = classifier(X)\n",
    "original_outputs.argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a584f0d-f415-4c3f-8286-5e8e75d90609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's predictions on the attacked data\n",
    "attacked_outputs = classifier(X_attacked)\n",
    "attacked_outputs.argmax(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f1642c-b9df-494d-ad07-895f67f39f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y true labels\n",
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9b0934-46ad-4089-a4c9-5fa5f3dd8cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model's predictions on the purified data\n",
    "classifier = classifier.eval()\n",
    "purified_outputs = classifier(purified_images)\n",
    "purified_outputs.argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1b8ee-b340-42ec-ae99-714a1ba73d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "end=time.time()\n",
    "print(\"temps final : \",end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3792887b-9b3e-46ac-9535-8b2cda9b10a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-neurealde",
   "language": "python",
   "name": "env-neurealde"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
